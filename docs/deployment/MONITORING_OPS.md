# ç›‘æ§è¿ç»´æ–‡æ¡£

> **æ–‡æ¡£ç›®çš„**: ä¸ºAIå¹¿å‘Šä»£æŠ•ç³»ç»Ÿæä¾›å…¨é¢çš„ç›‘æ§ã€è¿ç»´å’Œæ•…éšœå¤„ç†æŒ‡å—
> **ç›®æ ‡è¯»è€…**: DevOpså·¥ç¨‹å¸ˆã€è¿ç»´å›¢é˜Ÿã€ç³»ç»Ÿç®¡ç†å‘˜
> **æ›´æ–°æ—¥æœŸ**: 2025-11-11
> **ç‰ˆæœ¬**: v1.0

---

## ğŸ“‹ ç›®å½•

1. [ç›‘æ§æ¶æ„æ¦‚è§ˆ](#1-ç›‘æ§æ¶æ„æ¦‚è§ˆ)
2. [åº”ç”¨ç›‘æ§](#2-åº”ç”¨ç›‘æ§)
3. [åŸºç¡€è®¾æ–½ç›‘æ§](#3-åŸºç¡€è®¾æ–½ç›‘æ§)
4. [ä¸šåŠ¡æŒ‡æ ‡ç›‘æ§](#4-ä¸šåŠ¡æŒ‡æ ‡ç›‘æ§)
5. [æ—¥å¿—ç®¡ç†](#5-æ—¥å¿—ç®¡ç†)
6. [å‘Šè­¦ç³»ç»Ÿ](#6-å‘Šè­¦ç³»ç»Ÿ)
7. [æ•…éšœå¤„ç†](#7-æ•…éšœå¤„ç†)
8. [æ€§èƒ½ä¼˜åŒ–](#8-æ€§èƒ½ä¼˜åŒ–)
9. [å®¹é‡è§„åˆ’](#9-å®¹é‡è§„åˆ’)
10. [è¿ç»´è‡ªåŠ¨åŒ–](#10-è¿ç»´è‡ªåŠ¨åŒ–)

---

## 1. ç›‘æ§æ¶æ„æ¦‚è§ˆ

### 1.1 ç›‘æ§ä½“ç³»æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      æ•°æ®æ”¶é›†å±‚                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ åº”ç”¨æŒ‡æ ‡     â”‚ â”‚ åŸºç¡€è®¾æ–½æŒ‡æ ‡ â”‚ â”‚ ä¸šåŠ¡æŒ‡æ ‡     â”‚           â”‚
â”‚  â”‚ Prometheus  â”‚ â”‚ Node Exporterâ”‚ â”‚ è‡ªå®šä¹‰æŒ‡æ ‡   â”‚           â”‚
â”‚  â”‚ OpenTelemetryâ”‚ â”‚ cAdvisor     â”‚ â”‚ è‡ªå®šä¹‰Exporterâ”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      æ•°æ®å­˜å‚¨å±‚                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ Prometheus  â”‚              â”‚   Loki       â”‚               â”‚
â”‚  â”‚  æ—¶åºæ•°æ®åº“   â”‚              â”‚  æ—¥å¿—å­˜å‚¨    â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      å¯è§†åŒ–å±‚                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  Grafana    â”‚              â”‚ AlertManagerâ”‚               â”‚
â”‚  â”‚  æ•°æ®å¯è§†åŒ–  â”‚              â”‚  å‘Šè­¦ç®¡ç†    â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 ç›‘æ§æŒ‡æ ‡åˆ†ç±»

| æŒ‡æ ‡ç±»å‹ | ç›‘æ§å†…å®¹ | å·¥å…· | å‘Šè­¦çº§åˆ« |
|----------|----------|------|----------|
| **åº”ç”¨ç›‘æ§** | å“åº”æ—¶é—´ã€é”™è¯¯ç‡ã€ååé‡ | Prometheus + Grafana | P0 |
| **åŸºç¡€è®¾æ–½ç›‘æ§** | CPUã€å†…å­˜ã€ç£ç›˜ã€ç½‘ç»œ | Node Exporter | P1 |
| **æ•°æ®åº“ç›‘æ§** | è¿æ¥æ•°ã€æŸ¥è¯¢æ€§èƒ½ã€é”ç­‰å¾… | pg_exporter | P0 |
| **ä¸šåŠ¡ç›‘æ§** | ç”¨æˆ·æ´»è·ƒåº¦ã€è½¬åŒ–ç‡ã€æ”¶å…¥ | è‡ªå®šä¹‰æŒ‡æ ‡ | P0 |
| **å®‰å…¨ç›‘æ§** | ç™»å½•å¤±è´¥ã€å¼‚å¸¸è®¿é—®ã€æ”»å‡» | è‡ªå®šä¹‰æŒ‡æ ‡ | P0 |
| **æ—¥å¿—ç›‘æ§** | é”™è¯¯æ—¥å¿—ã€å¼‚å¸¸å †æ ˆ | Loki + Grafana | P1 |

---

## 2. åº”ç”¨ç›‘æ§

### 2.1 åº”ç”¨æŒ‡æ ‡é…ç½®

```python
# backend/app/monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from fastapi import Request, Response
import time
import logging

# åº”ç”¨æŒ‡æ ‡
REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status_code']
)

REQUEST_DURATION = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration in seconds',
    ['method', 'endpoint'],
    buckets=[0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
)

ACTIVE_USERS = Gauge(
    'active_users_total',
    'Number of active users'
)

DATABASE_CONNECTIONS = Gauge(
    'database_connections_active',
    'Active database connections'
)

BUSINESS_METRICS = {
    'projects_created_total': Counter(
        'projects_created_total',
        'Total number of projects created',
        ['user_role', 'client_type']
    ),
    'daily_reports_submitted': Counter(
        'daily_reports_submitted_total',
        'Total number of daily reports submitted'
    ),
    'recharge_requests_total': Counter(
        'recharge_requests_total',
        'Total number of recharge requests',
        ['status', 'amount_range']
    )
}

# ç›‘æ§ä¸­é—´ä»¶
async def monitoring_middleware(request: Request, call_next):
    start_time = time.time()

    # å¤„ç†è¯·æ±‚
    response = await call_next(request)

    # è®°å½•è¯·æ±‚æŒ‡æ ‡
    method = request.method
    endpoint = request.url.path
    status_code = str(response.status_code)
    duration = time.time() - start_time

    REQUEST_COUNT.labels(method, endpoint, status_code).inc()
    REQUEST_DURATION.labels(method, endpoint).observe(duration)

    return response

# ä¸šåŠ¡æŒ‡æ ‡æ›´æ–°
class BusinessMetrics:
    @staticmethod
    def record_project_created(user_role: str, client_type: str):
        """è®°å½•é¡¹ç›®åˆ›å»º"""
        BUSINESS_METRICS['projects_created_total'].labels(
            user_role=user_role,
            client_type=client_type
        ).inc()

    @staticmethod
    def record_daily_report_submitted():
        """è®°å½•æ—¥æŠ¥æäº¤"""
        BUSINESS_METRICS['daily_reports_submitted'].inc()

    @staticmethod
    def record_recharge_request(status: str, amount: float):
        """è®°å½•å……å€¼è¯·æ±‚"""
        # ç¡®å®šé‡‘é¢èŒƒå›´
        if amount < 1000:
            amount_range = 'small'
        elif amount < 10000:
            amount_range = 'medium'
        else:
            amount_range = 'large'

        BUSINESS_METRICS['recharge_requests_total'].labels(
            status=status,
            amount_range=amount_range
        ).inc()

# å¥åº·æ£€æŸ¥å’ŒæŒ‡æ ‡ç«¯ç‚¹
from fastapi import FastAPI
from fastapi.responses import PlainTextResponse

app = FastAPI()

@app.get("/metrics")
async def metrics():
    """PrometheusæŒ‡æ ‡ç«¯ç‚¹"""
    return PlainTextResponse(generate_latest())

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    # æ£€æŸ¥æ•°æ®åº“è¿æ¥
    db_status = await check_database_health()

    # æ£€æŸ¥Redisè¿æ¥
    redis_status = await check_redis_health()

    # æ£€æŸ¥å¤–éƒ¨APIè¿æ¥
    external_api_status = await check_external_api_health()

    overall_status = "healthy" if all([
        db_status, redis_status, external_api_status
    ]) else "unhealthy"

    return {
        "status": overall_status,
        "services": {
            "database": "healthy" if db_status else "unhealthy",
            "redis": "healthy" if redis_status else "unhealthy",
            "external_api": "healthy" if external_api_status else "unhealthy"
        },
        "timestamp": time.time()
    }

async def check_database_health():
    """æ£€æŸ¥æ•°æ®åº“å¥åº·çŠ¶æ€"""
    try:
        db = SessionLocal()
        db.execute("SELECT 1")
        db.close()
        return True
    except Exception as e:
        logging.error(f"Database health check failed: {e}")
        return False

async def check_redis_health():
    """æ£€æŸ¥Rediså¥åº·çŠ¶æ€"""
    try:
        await redis_client.ping()
        return True
    except Exception as e:
        logging.error(f"Redis health check failed: {e}")
        return False

async def check_external_api_health():
    """æ£€æŸ¥å¤–éƒ¨APIå¥åº·çŠ¶æ€"""
    try:
        response = requests.get("https://graph.facebook.com/v18.0/", timeout=5)
        return response.status_code == 200
    except Exception as e:
        logging.error(f"External API health check failed: {e}")
        return False
```

### 2.2 è‡ªå®šä¹‰ä¸šåŠ¡æŒ‡æ ‡

```python
# backend/app/monitoring/business_metrics.py
from prometheus_client import Counter, Histogram, Gauge, info
from datetime import datetime, timedelta
import asyncio

# ä¸šåŠ¡æŒ‡æ ‡
USER_METRICS = {
    'login_attempts': Counter('user_login_attempts_total', 'Total login attempts', ['status']),
    'user_registrations': Counter('user_registrations_total', 'Total user registrations', ['role']),
    'active_sessions': Gauge('user_active_sessions', 'Number of active user sessions'),
}

FINANCIAL_METRICS = {
    'total_spend': Counter('ad_spend_total', 'Total ad spend', ['project_id', 'date']),
    'recharge_amount': Counter('recharge_amount_total', 'Total recharge amount', ['status']),
    'conversion_value': Counter('conversion_value_total', 'Total conversion value', ['project_id']),
}

PERFORMANCE_METRICS = {
    'report_processing_time': Histogram(
        'report_processing_duration_seconds',
        'Time to process daily reports',
        buckets=[1.0, 5.0, 10.0, 30.0, 60.0, 300.0]
    ),
    'data_sync_duration': Histogram(
        'data_sync_duration_seconds',
        'Time to sync data with external APIs',
        buckets=[5.0, 15.0, 30.0, 60.0, 300.0, 600.0]
    )
}

class BusinessMonitor:
    def __init__(self, db_session, redis_client):
        self.db = db_session
        self.redis = redis_client

    async def collect_daily_metrics(self):
        """æ”¶é›†æ¯æ—¥ä¸šåŠ¡æŒ‡æ ‡"""
        today = datetime.now().date()

        # æ”¶é›†ç”¨æˆ·æ´»è·ƒåº¦
        await self._collect_user_activity(today)

        # æ”¶é›†è´¢åŠ¡æŒ‡æ ‡
        await self._collect_financial_metrics(today)

        # æ”¶é›†é¡¹ç›®çŠ¶æ€
        await self._collect_project_metrics(today)

    async def _collect_user_activity(self, date):
        """æ”¶é›†ç”¨æˆ·æ´»è·ƒåº¦æŒ‡æ ‡"""
        try:
            # ä»Šæ—¥ç™»å½•ç”¨æˆ·æ•°
            login_count = await self.redis.get(f"daily_logins:{date}") or 0

            # æ´»è·ƒç”¨æˆ·æ•°
            active_users = self.db.query(User).filter(
                User.last_login >= date
            ).count()

            # æ›´æ–°æŒ‡æ ‡
            USER_METRICS['active_sessions'].set(active_users)

            logging.info(f"User activity metrics collected for {date}: "
                        f"logins={login_count}, active_users={active_users}")

        except Exception as e:
            logging.error(f"Failed to collect user activity metrics: {e}")

    async def _collect_financial_metrics(self, date):
        """æ”¶é›†è´¢åŠ¡æŒ‡æ ‡"""
        try:
            # ä»Šæ—¥æ¶ˆè€—
            today_spend = self.db.query(func.sum(DailyReport.spend)).filter(
                DailyReport.report_date == date
            ).scalar() or 0

            # ä»Šæ—¥å……å€¼
            today_recharge = self.db.query(func.sum(RechargeRequest.amount)).filter(
                RechargeRequest.created_at >= date,
                RechargeRequest.status == 'approved'
            ).scalar() or 0

            # æ›´æ–°æŒ‡æ ‡
            FINANCIAL_METRICS['total_spend'].labels(project_id='all', date=str(date)).inc(today_spend)
            FINANCIAL_METRICS['recharge_amount'].labels(status='approved').inc(today_recharge)

            logging.info(f"Financial metrics collected for {date}: "
                        f"spend={today_spend}, recharge={today_recharge}")

        except Exception as e:
            logging.error(f"Failed to collect financial metrics: {e}")

    async def _collect_project_metrics(self, date):
        """æ”¶é›†é¡¹ç›®æŒ‡æ ‡"""
        try:
            # é¡¹ç›®æ€»æ•°
            total_projects = self.db.query(Project).count()

            # æ´»è·ƒé¡¹ç›®æ•°
            active_projects = self.db.query(Project).filter(
                Project.status == 'active'
            ).count()

            # ä»Šæ—¥æ–°å¢é¡¹ç›®
            new_projects_today = self.db.query(Project).filter(
                Project.created_at >= date
            ).count()

            logging.info(f"Project metrics collected for {date}: "
                        f"total={total_projects}, active={active_projects}, "
                        f"new_today={new_projects_today}")

        except Exception as e:
            logging.error(f"Failed to collect project metrics: {e}")

# å®šæ—¶ä»»åŠ¡ï¼šæ¯å°æ—¶æ”¶é›†ä¸€æ¬¡æŒ‡æ ‡
async def schedule_metrics_collection():
    """è°ƒåº¦æŒ‡æ ‡æ”¶é›†"""
    monitor = BusinessMonitor(db_session, redis_client)

    while True:
        try:
            await monitor.collect_daily_metrics()
            await asyncio.sleep(3600)  # æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡
        except Exception as e:
            logging.error(f"Error in metrics collection: {e}")
            await asyncio.sleep(300)  # å‡ºé”™æ—¶5åˆ†é’Ÿåé‡è¯•
```

---

## 3. åŸºç¡€è®¾æ–½ç›‘æ§

### 3.1 æœåŠ¡å™¨ç›‘æ§é…ç½®

```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  # åº”ç”¨æœåŠ¡ç›‘æ§
  - job_name: 'ai-ad-spend-backend'
    static_configs:
      - targets: ['backend:8000']
    metrics_path: '/metrics'
    scrape_interval: 15s
    scrape_timeout: 10s

  - job_name: 'ai-ad-spend-frontend'
    static_configs:
      - targets: ['frontend:3000']
    metrics_path: '/api/metrics'
    scrape_interval: 30s

  # Node Exporter - ç³»ç»ŸæŒ‡æ ‡
  - job_name: 'node-exporter'
    static_configs:
      - targets:
        - 'node-exporter:9100'
    scrape_interval: 10s

  # cAdvisor - å®¹å™¨æŒ‡æ ‡
  - job_name: 'cadvisor'
    static_configs:
      - targets:
        - 'cadvisor:8080'
    scrape_interval: 15s

  # PostgreSQL Exporter
  - job_name: 'postgres-exporter'
    static_configs:
      - targets:
        - 'postgres-exporter:9187'
    scrape_interval: 15s

  # Redis Exporter
  - job_name: 'redis-exporter'
    static_configs:
      - targets:
        - 'redis-exporter:9121'
    scrape_interval: 15s

  # Nginx Exporter
  - job_name: 'nginx-exporter'
    static_configs:
      - targets:
        - 'nginx-exporter:9113'
    scrape_interval: 15s
```

### 3.2 Docker Composeç›‘æ§é…ç½®

```yaml
# docker-compose.monitoring.yml
version: '3.8'

services:
  # Prometheus
  prometheus:
    image: prom/prometheus:latest
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./monitoring/alert_rules.yml:/etc/prometheus/alert_rules.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    networks:
      - monitoring

  # Grafana
  grafana:
    image: grafana/grafana:latest
    restart: unless-stopped
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
    networks:
      - monitoring

  # AlertManager
  alertmanager:
    image: prom/alertmanager:latest
    restart: unless-stopped
    ports:
      - "9093:9093"
    volumes:
      - ./monitoring/alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager_data:/alertmanager
    networks:
      - monitoring

  # Node Exporter
  node-exporter:
    image: prom/node-exporter:latest
    restart: unless-stopped
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    networks:
      - monitoring

  # cAdvisor
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    restart: unless-stopped
    ports:
      - "8080:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    privileged: true
    devices:
      - /dev/kmsg
    networks:
      - monitoring

  # PostgreSQL Exporter
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    restart: unless-stopped
    ports:
      - "9187:9187"
    environment:
      - DATA_SOURCE_NAME=postgresql://postgres:password@postgres:5432/ai_ad_spend_prod?sslmode=disable
    networks:
      - monitoring

  # Redis Exporter
  redis-exporter:
    image: oliver006/redis_exporter:latest
    restart: unless-stopped
    ports:
      - "9121:9121"
    environment:
      - REDIS_ADDR=redis://redis:6379
      - REDIS_PASSWORD=password
    networks:
      - monitoring

  # Loki (æ—¥å¿—èšåˆ)
  loki:
    image: grafana/loki:latest
    restart: unless-stopped
    ports:
      - "3100:3100"
    volumes:
      - ./monitoring/loki.yml:/etc/loki/local-config.yaml
      - loki_data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - monitoring

  # Promtail (æ—¥å¿—æ”¶é›†)
  promtail:
    image: grafana/promtail:latest
    restart: unless-stopped
    volumes:
      - ./monitoring/promtail.yml:/etc/promtail/config.yml
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    command: -config.file=/etc/promtail/config.yml
    networks:
      - monitoring

volumes:
  prometheus_data:
  grafana_data:
  alertmanager_data:
  loki_data:

networks:
  monitoring:
    driver: bridge
```

---

## 4. ä¸šåŠ¡æŒ‡æ ‡ç›‘æ§

### 4.1 Grafanaä»ªè¡¨ç›˜é…ç½®

```json
{
  "dashboard": {
    "id": null,
    "title": "AIå¹¿å‘Šä»£æŠ•ç³»ç»Ÿ - ä¸šåŠ¡ç›‘æ§",
    "tags": ["ai-ad-spend", "business"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "é¡¹ç›®æ¦‚è§ˆ",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(ai_ad_spend_projects_total)",
            "legendFormat": "æ€»é¡¹ç›®æ•°"
          },
          {
            "expr": "sum(ai_ad_spend_active_projects_total)",
            "legendFormat": "æ´»è·ƒé¡¹ç›®æ•°"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {"mode": "palette-classic"},
            "custom": {"displayMode": "list", "orientation": "horizontal"},
            "mappings": [],
            "thresholds": {
              "steps": [
                {"color": "green", "value": null},
                {"color": "red", "value": 80}
              ]
            }
          }
        }
      },
      {
        "id": 2,
        "title": "ç”¨æˆ·æ´»è·ƒåº¦",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(user_login_attempts_total{status=\"success\"}[5m])",
            "legendFormat": "æˆåŠŸç™»å½•"
          },
          {
            "expr": "rate(user_login_attempts_total{status=\"failed\"}[5m])",
            "legendFormat": "å¤±è´¥ç™»å½•"
          },
          {
            "expr": "user_active_sessions",
            "legendFormat": "æ´»è·ƒä¼šè¯"
          }
        ]
      },
      {
        "id": 3,
        "title": "è´¢åŠ¡æŒ‡æ ‡",
        "type": "graph",
        "targets": [
          {
            "expr": "increase(ad_spend_total[1h])",
            "legendFormat": "æ¯å°æ—¶æ¶ˆè€—"
          },
          {
            "expr": "increase(recharge_amount_total{status=\"approved\"}[1h])",
            "legendFormat": "æ¯å°æ—¶å……å€¼"
          },
          {
            "expr": "increase(conversion_value_total[1h])",
            "legendFormat": "æ¯å°æ—¶è½¬åŒ–ä»·å€¼"
          }
        ]
      },
      {
        "id": 4,
        "title": "APIæ€§èƒ½",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95thç™¾åˆ†ä½å“åº”æ—¶é—´"
          },
          {
            "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "50thç™¾åˆ†ä½å“åº”æ—¶é—´"
          }
        ]
      },
      {
        "id": 5,
        "title": "é”™è¯¯ç‡",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total{status_code=~\"5..\"}[5m]) / rate(http_requests_total[5m])",
            "legendFormat": "5xxé”™è¯¯ç‡"
          },
          {
            "expr": "rate(http_requests_total{status_code=~\"4..\"}[5m]) / rate(http_requests_total[5m])",
            "legendFormat": "4xxé”™è¯¯ç‡"
          }
        ]
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "5s"
  }
}
```

### 4.2 è‡ªå®šä¹‰ä¸šåŠ¡æŒ‡æ ‡æ”¶é›†å™¨

```python
# backend/app/monitoring/business_collector.py
from prometheus_client import Gauge, Counter, CollectorRegistry, generate_latest
import asyncio
import logging
from datetime import datetime, timedelta
from sqlalchemy import func

class BusinessMetricsCollector:
    def __init__(self, db_session):
        self.db = db_session
        self.registry = CollectorRegistry()
        self._setup_metrics()

    def _setup_metrics(self):
        """è®¾ç½®ä¸šåŠ¡æŒ‡æ ‡"""
        # é¡¹ç›®ç›¸å…³æŒ‡æ ‡
        self.project_metrics = {
            'total_projects': Gauge(
                'business_total_projects',
                'Total number of projects',
                registry=self.registry
            ),
            'active_projects': Gauge(
                'business_active_projects',
                'Number of active projects',
                registry=self.registry
            ),
            'projects_by_status': Gauge(
                'business_projects_by_status',
                'Projects by status',
                ['status'],
                registry=self.registry
            )
        }

        # è´¢åŠ¡ç›¸å…³æŒ‡æ ‡
        self.financial_metrics = {
            'daily_spend': Gauge(
                'business_daily_spend',
                'Total ad spend for the day',
                ['date'],
                registry=self.registry
            ),
            'monthly_budget_utilization': Gauge(
                'business_monthly_budget_utilization',
                'Monthly budget utilization percentage',
                ['project_id'],
                registry=self.registry
            ),
            'recharge_pending_amount': Gauge(
                'business_recharge_pending_amount',
                'Total pending recharge amount',
                registry=self.registry
            )
        }

        # ç”¨æˆ·ç›¸å…³æŒ‡æ ‡
        self.user_metrics = {
            'total_users': Gauge(
                'business_total_users',
                'Total number of users',
                ['role'],
                registry=self.registry
            ),
            'daily_active_users': Gauge(
                'business_daily_active_users',
                'Daily active users',
                registry=self.registry
            )
        }

    async def collect_all_metrics(self):
        """æ”¶é›†æ‰€æœ‰ä¸šåŠ¡æŒ‡æ ‡"""
        try:
            await self._collect_project_metrics()
            await self._collect_financial_metrics()
            await self._collect_user_metrics()

            logging.info("Business metrics collected successfully")

        except Exception as e:
            logging.error(f"Failed to collect business metrics: {e}")

    async def _collect_project_metrics(self):
        """æ”¶é›†é¡¹ç›®æŒ‡æ ‡"""
        # æ€»é¡¹ç›®æ•°
        total_projects = self.db.query(Project).count()
        self.project_metrics['total_projects'].set(total_projects)

        # æ´»è·ƒé¡¹ç›®æ•°
        active_projects = self.db.query(Project).filter(
            Project.status == 'active'
        ).count()
        self.project_metrics['active_projects'].set(active_projects)

        # æŒ‰çŠ¶æ€åˆ†ç±»çš„é¡¹ç›®æ•°
        status_counts = self.db.query(
            Project.status,
            func.count(Project.id)
        ).group_by(Project.status).all()

        for status, count in status_counts:
            self.project_metrics['projects_by_status'].labels(status=status).set(count)

    async def _collect_financial_metrics(self):
        """æ”¶é›†è´¢åŠ¡æŒ‡æ ‡"""
        today = datetime.now().date()

        # ä»Šæ—¥æ¶ˆè€—
        today_spend = self.db.query(func.sum(DailyReport.spend)).filter(
            DailyReport.report_date == today
        ).scalar() or 0

        self.financial_metrics['daily_spend'].labels(date=str(today)).set(today_spend)

        # æœˆåº¦é¢„ç®—åˆ©ç”¨ç‡
        current_month_start = today.replace(day=1)
        project_budgets = self.db.query(
            Project.id,
            Project.budget,
            func.sum(DailyReport.spend).label('current_spend')
        ).outerjoin(
            DailyReport,
            Project.id == DailyReport.project_id
        ).filter(
            DailyReport.report_date >= current_month_start
        ).group_by(Project.id, Project.budget).all()

        for project_id, budget, current_spend in project_budgets:
            utilization = (current_spend or 0) / budget if budget > 0 else 0
            self.financial_metrics['monthly_budget_utilization'].labels(
                project_id=str(project_id)
            ).set(utilization)

        # å¾…å¤„ç†å……å€¼é‡‘é¢
        pending_amount = self.db.query(func.sum(RechargeRequest.amount)).filter(
            RechargeRequest.status == 'pending'
        ).scalar() or 0

        self.financial_metrics['recharge_pending_amount'].set(pending_amount)

    async def _collect_user_metrics(self):
        """æ”¶é›†ç”¨æˆ·æŒ‡æ ‡"""
        # æŒ‰è§’è‰²ç»Ÿè®¡ç”¨æˆ·æ•°
        role_counts = self.db.query(
            User.role,
            func.count(User.id)
        ).group_by(User.role).all()

        for role, count in role_counts:
            self.user_metrics['total_users'].labels(role=role).set(count)

        # ä»Šæ—¥æ´»è·ƒç”¨æˆ·
        today = datetime.now().date()
        active_users = self.db.query(User).filter(
            User.last_login >= today
        ).count()

        self.user_metrics['daily_active_users'].set(active_users)

    def get_metrics_output(self):
        """è·å–æŒ‡æ ‡è¾“å‡º"""
        return generate_latest(self.registry)

# FastAPIè·¯ç”±é›†æˆ
from fastapi import APIRouter
from fastapi.responses import PlainTextResponse

router = APIRouter()

@router.get("/business-metrics")
async def business_metrics():
    """ä¸šåŠ¡æŒ‡æ ‡ç«¯ç‚¹"""
    collector = BusinessMetricsCollector(db_session)
    await collector.collect_all_metrics()
    return PlainTextResponse(collector.get_metrics_output())
```

---

## 5. æ—¥å¿—ç®¡ç†

### 5.1 æ—¥å¿—é…ç½®

```python
# backend/app/logging_config.py
import logging
import logging.config
import json
import sys
from datetime import datetime
from pathlib import Path

# æ—¥å¿—é…ç½®
LOGGING_CONFIG = {
    "version": 1,
    "disable_existing_loggers": False,
    "formatters": {
        "detailed": {
            "()": "python.logging.Formatter",
            "format": "%(asctime)s [%(levelname)s] %(name)s: %(message)s",
            "datefmt": "%Y-%m-%d %H:%M:%S"
        },
        "json": {
            "()": "pythonlogging_json.JsonFormatter",
            "format": "%(asctime)s %(name)s %(levelname)s %(message)s"
        }
    },
    "handlers": {
        "console": {
            "class": "logging.StreamHandler",
            "level": "INFO",
            "formatter": "detailed",
            "stream": sys.stdout
        },
        "file": {
            "class": "logging.handlers.RotatingFileHandler",
            "level": "DEBUG",
            "formatter": "json",
            "filename": "/var/log/ai-ad-spend/app.log",
            "maxBytes": 10485760,  # 10MB
            "backupCount": 5,
            "encoding": "utf8"
        },
        "error_file": {
            "class": "logging.handlers.RotatingFileHandler",
            "level": "ERROR",
            "formatter": "json",
            "filename": "/var/log/ai-ad-spend/error.log",
            "maxBytes": 10485760,  # 10MB
            "backupCount": 5,
            "encoding": "utf8"
        },
        "security_file": {
            "class": "logging.handlers.RotatingFileHandler",
            "level": "INFO",
            "formatter": "json",
            "filename": "/var/log/ai-ad-spend/security.log",
            "maxBytes": 10485760,  # 10MB
            "backupCount": 5,
            "encoding": "utf8"
        }
    },
    "loggers": {
        "": {
            "level": "INFO",
            "handlers": ["console", "file", "error_file"]
        },
        "app.security": {
            "level": "INFO",
            "handlers": ["security_file"],
            "propagate": False
        },
        "app.database": {
            "level": "WARNING",
            "handlers": ["file"],
            "propagate": True
        },
        "uvicorn": {
            "level": "INFO",
            "handlers": ["console"],
            "propagate": False
        }
    }
}

# ç»“æ„åŒ–æ—¥å¿—è®°å½•å™¨
class StructuredLogger:
    def __init__(self, name: str):
        self.logger = logging.getLogger(name)

    def info(self, message: str, **kwargs):
        """è®°å½•ä¿¡æ¯æ—¥å¿—"""
        extra_data = {
            "timestamp": datetime.utcnow().isoformat(),
            "service": "ai-ad-spend",
            **kwargs
        }
        self.logger.info(message, extra={"extra_data": extra_data})

    def error(self, message: str, error: Exception = None, **kwargs):
        """è®°å½•é”™è¯¯æ—¥å¿—"""
        extra_data = {
            "timestamp": datetime.utcnow().isoformat(),
            "service": "ai-ad-spend",
            "error_type": type(error).__name__ if error else None,
            "error_message": str(error) if error else None,
            **kwargs
        }
        self.logger.error(message, extra={"extra_data": extra_data})

    def security(self, event: str, **kwargs):
        """è®°å½•å®‰å…¨äº‹ä»¶æ—¥å¿—"""
        extra_data = {
            "timestamp": datetime.utcnow().isoformat(),
            "service": "ai-ad-spend",
            "event_type": "security",
            "security_event": event,
            **kwargs
        }
        self.logger.info(f"SECURITY: {event}", extra={"extra_data": extra_data})

    def audit(self, action: str, user_id: str = None, resource: str = None, **kwargs):
        """è®°å½•å®¡è®¡æ—¥å¿—"""
        extra_data = {
            "timestamp": datetime.utcnow().isoformat(),
            "service": "ai-ad-spend",
            "event_type": "audit",
            "action": action,
            "user_id": user_id,
            "resource": resource,
            **kwargs
        }
        self.logger.info(f"AUDIT: {action}", extra={"extra_data": extra_data})

# ä½¿ç”¨ç¤ºä¾‹
logger = StructuredLogger("app.main")

logger.info("APIè¯·æ±‚å¤„ç†",
           method="POST",
           endpoint="/api/projects",
           user_id="user-123",
           request_id="req-456")

logger.error("æ•°æ®åº“è¿æ¥å¤±è´¥",
            error=Exception("Connection timeout"),
            retry_count=3)

logger.security("ç™»å½•å¤±è´¥",
               user_id="user-123",
               ip_address="192.168.1.100",
               reason="å¯†ç é”™è¯¯")

logger.audit("é¡¹ç›®åˆ›å»º",
             user_id="user-123",
             resource="project-456",
             project_name="æ–°é¡¹ç›®")
```

### 5.2 Lokiæ—¥å¿—èšåˆé…ç½®

```yaml
# monitoring/loki.yml
auth_enabled: false

server:
  http_listen_port: 3100

ingester:
  lifecycler:
    address: 127.0.0.1
    ring:
      kvstore:
        store: inmemory
      replication_factor: 1
    final_sleep: 0s
  chunk_idle_period: 1h
  max_chunk_age: 1h
  chunk_target_size: 1048576
  chunk_retain_period: 30s

schema_config:
  configs:
    - from: 2020-10-24
      store: boltdb-shipper
      object_store: filesystem
      schema: v11
      index:
        prefix: index_
        period: 24h

storage_config:
  boltdb_shipper:
    active_index_directory: /loki/boltdb-shipper-active
    cache_location: /loki/boltdb-shipper-cache
    shared_store: filesystem
  filesystem:
    directory: /loki/chunks

limits_config:
  enforce_metric_name: false
  reject_old_samples: true
  reject_old_samples_max_age: 168h

chunk_store_config:
  max_look_back_period: 0s

table_manager:
  retention_deletes_enabled: false
  retention_period: 0s
```

### 5.3 Promtailæ—¥å¿—æ”¶é›†é…ç½®

```yaml
# monitoring/promtail.yml
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  - job_name: containers
    static_configs:
      - targets:
          - localhost
        labels:
          job: containerlogs
          __path__: /var/lib/docker/containers/*/*log

    pipeline_stages:
      - json:
          expressions:
            output: log
            stream: stream
            attrs:
      - json:
          expressions:
            tag:
          source: attrs
      - regex:
          expression: (?P<container_name>(?:[^|]*))\|
          source: tag
      - timestamp:
          format: RFC3339Nano
          source: time
      - labels:
          stream:
          container_name:
      - output:
          source: output

  - job_name: system_logs
    static_configs:
      - targets:
          - localhost
        labels:
          job: varlogs
          __path__: /var/log/*log

    pipeline_stages:
      - regex:
          expression: (?P<timestamp>\w+\s+\d+\s+\d+:\d+:\d+)\s+(?P<level>\w+)\s+(?P<message>.*)
      - timestamp:
          format: Jan 02 15:04:05
          source: timestamp
      - labels:
          level:
      - output:
          source: message
```

---

## 6. å‘Šè­¦ç³»ç»Ÿ

### 6.1 å‘Šè­¦è§„åˆ™é…ç½®

```yaml
# monitoring/alert_rules.yml
groups:
  - name: application.rules
    rules:
      # åº”ç”¨é”™è¯¯ç‡å‘Šè­¦
      - alert: HighErrorRate
        expr: rate(http_requests_total{status_code=~"5.."}[5m]) > 0.05
        for: 2m
        labels:
          severity: warning
          service: ai-ad-spend
        annotations:
          summary: "åº”ç”¨é”™è¯¯ç‡è¿‡é«˜"
          description: "5åˆ†é’Ÿå†…5xxé”™è¯¯ç‡è¶…è¿‡5%ï¼Œå½“å‰å€¼: {{ $value | humanizePercentage }}"

      - alert: CriticalErrorRate
        expr: rate(http_requests_total{status_code=~"5.."}[5m]) > 0.15
        for: 1m
        labels:
          severity: critical
          service: ai-ad-spend
        annotations:
          summary: "åº”ç”¨é”™è¯¯ç‡ä¸¥é‡"
          description: "5åˆ†é’Ÿå†…5xxé”™è¯¯ç‡è¶…è¿‡15%ï¼Œå½“å‰å€¼: {{ $value | humanizePercentage }}"

      # å“åº”æ—¶é—´å‘Šè­¦
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
        for: 3m
        labels:
          severity: warning
          service: ai-ad-spend
        annotations:
          summary: "åº”ç”¨å“åº”æ—¶é—´è¿‡é•¿"
          description: "95%è¯·æ±‚å“åº”æ—¶é—´è¶…è¿‡2ç§’ï¼Œå½“å‰å€¼: {{ $value }}s"

      - alert: CriticalResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 5
        for: 1m
        labels:
          severity: critical
          service: ai-ad-spend
        annotations:
          summary: "åº”ç”¨å“åº”æ—¶é—´ä¸¥é‡è¶…æ ‡"
          description: "95%è¯·æ±‚å“åº”æ—¶é—´è¶…è¿‡5ç§’ï¼Œå½“å‰å€¼: {{ $value }}s"

      # æ•°æ®åº“å‘Šè­¦
      - alert: DatabaseDown
        expr: up{job="postgres-exporter"} == 0
        for: 1m
        labels:
          severity: critical
          service: database
        annotations:
          summary: "æ•°æ®åº“æœåŠ¡å®•æœº"
          description: "PostgreSQLæ•°æ®åº“æ— æ³•è®¿é—®"

      - alert: HighDatabaseConnections
        expr: pg_stat_activity_count > 80
        for: 5m
        labels:
          severity: warning
          service: database
        annotations:
          summary: "æ•°æ®åº“è¿æ¥æ•°è¿‡é«˜"
          description: "å½“å‰æ´»è·ƒè¿æ¥æ•°: {{ $value }}"

      - alert: DatabaseSlowQueries
        expr: rate(pg_stat_statements_mean_time_seconds[5m]) > 1
        for: 5m
        labels:
          severity: warning
          service: database
        annotations:
          summary: "æ•°æ®åº“æ…¢æŸ¥è¯¢"
          description: "å¹³å‡æŸ¥è¯¢æ—¶é—´è¶…è¿‡1ç§’: {{ $value }}s"

      # Rediså‘Šè­¦
      - alert: RedisDown
        expr: up{job="redis-exporter"} == 0
        for: 1m
        labels:
          severity: critical
          service: cache
        annotations:
          summary: "RedisæœåŠ¡å®•æœº"
          description: "Redisç¼“å­˜æœåŠ¡æ— æ³•è®¿é—®"

      - alert: RedisMemoryHigh
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
        for: 5m
        labels:
          severity: warning
          service: cache
        annotations:
          summary: "Rediså†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
          description: "Rediså†…å­˜ä½¿ç”¨ç‡: {{ $value | humanizePercentage }}"

  - name: business.rules
    rules:
      # ä¸šåŠ¡æŒ‡æ ‡å‘Šè­¦
      - alert: NoNewProjects
        expr: increase(business_projects_created_total[24h]) == 0
        for: 12h
        labels:
          severity: warning
          service: business
        annotations:
          summary: "24å°æ—¶å†…æ— æ–°é¡¹ç›®åˆ›å»º"
          description: "ç³»ç»Ÿå¯èƒ½å­˜åœ¨ä¸šåŠ¡å¼‚å¸¸"

      - alert: HighFailedLogins
        expr: rate(user_login_attempts_total{status="failed"}[5m]) > 5
        for: 2m
        labels:
          severity: warning
          service: security
        annotations:
          summary: "é«˜é¢‘ç™»å½•å¤±è´¥"
          description: "5åˆ†é’Ÿå†…ç™»å½•å¤±è´¥ç‡è¿‡é«˜: {{ $value }}æ¬¡/ç§’"

      - alert: PendingRechargeHigh
        expr: business_recharge_pending_amount > 50000
        for: 30m
        labels:
          severity: warning
          service: business
        annotations:
          summary: "å¾…å¤„ç†å……å€¼é‡‘é¢è¿‡é«˜"
          description: "å½“å‰å¾…å¤„ç†å……å€¼é‡‘é¢: Â¥{{ $value }}"

  - name: infrastructure.rules
    rules:
      # ç³»ç»Ÿèµ„æºå‘Šè­¦
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
          service: infrastructure
        annotations:
          summary: "CPUä½¿ç”¨ç‡è¿‡é«˜"
          description: "å®ä¾‹ {{ $labels.instance }} CPUä½¿ç”¨ç‡: {{ $value }}%"

      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
          service: infrastructure
        annotations:
          summary: "å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
          description: "å®ä¾‹ {{ $labels.instance }} å†…å­˜ä½¿ç”¨ç‡: {{ $value }}%"

      - alert: DiskSpaceLow
        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 90
        for: 5m
        labels:
          severity: critical
          service: infrastructure
        annotations:
          summary: "ç£ç›˜ç©ºé—´ä¸è¶³"
          description: "å®ä¾‹ {{ $labels.instance }} ç£ç›˜ {{ $labels.mountpoint }} ä½¿ç”¨ç‡: {{ $value }}%"

      - alert: DiskSpaceWarning
        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 80
        for: 10m
        labels:
          severity: warning
          service: infrastructure
        annotations:
          summary: "ç£ç›˜ç©ºé—´é¢„è­¦"
          description: "å®ä¾‹ {{ $labels.instance }} ç£ç›˜ {{ $labels.mountpoint }} ä½¿ç”¨ç‡: {{ $value }}%"
```

### 6.2 AlertManageré…ç½®

```yaml
# monitoring/alertmanager.yml
global:
  smtp_smarthost: 'smtp.company.com:587'
  smtp_from: 'alerts@company.com'
  smtp_auth_username: 'alerts@company.com'
  smtp_auth_password: 'smtp-password'

templates:
  - '/etc/alertmanager/templates/*.tmpl'

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'default'
  routes:
    # ä¸¥é‡å‘Šè­¦ç«‹å³é€šçŸ¥
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 0s
      repeat_interval: 5m

    # å®‰å…¨å‘Šè­¦ç‰¹æ®Šå¤„ç†
    - match:
        service: security
      receiver: 'security-alerts'
      group_wait: 0s
      repeat_interval: 30m

    # ä¸šåŠ¡å‘Šè­¦
    - match:
        service: business
      receiver: 'business-alerts'
      repeat_interval: 2h

receivers:
  # é»˜è®¤æ¥æ”¶è€…
  - name: 'default'
    email_configs:
      - to: 'ops@company.com'
        subject: '[{{ .Status | toUpper }}] {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          å‘Šè­¦: {{ .Annotations.summary }}
          æè¿°: {{ .Annotations.description }}
          æ ‡ç­¾: {{ range .Labels.SortedPairs }}{{ .Name }}={{ .Value }} {{ end }}
          æ—¶é—´: {{ .StartsAt }}
          {{ end }}

  # ä¸¥é‡å‘Šè­¦
  - name: 'critical-alerts'
    email_configs:
      - to: 'critical-alerts@company.com'
        subject: '[CRITICAL] {{ .GroupLabels.alertname }}'
        body: |
          ç´§æ€¥å‘Šè­¦é€šçŸ¥ï¼
          {{ range .Alerts }}
          å‘Šè­¦: {{ .Annotations.summary }}
          æè¿°: {{ .Annotations.description }}
          æ—¶é—´: {{ .StartsAt }}
          ç«‹å³å¤„ç†ï¼
          {{ end }}
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#critical-alerts'
        title: 'ğŸš¨ Critical Alert'
        text: |
          {{ range .Alerts }}
          {{ .Annotations.summary }}
          {{ .Annotations.description }}
          {{ end }}

  # å®‰å…¨å‘Šè­¦
  - name: 'security-alerts'
    email_configs:
      - to: 'security-team@company.com'
        subject: '[SECURITY] {{ .GroupLabels.alertname }}'
    webhook_configs:
      - url: 'http://security-alert-handler:8080/webhook'

  # ä¸šåŠ¡å‘Šè­¦
  - name: 'business-alerts'
    email_configs:
      - to: 'business-team@company.com'
        subject: '[BUSINESS] {{ .GroupLabels.alertname }}'

# å‘Šè­¦æŠ‘åˆ¶è§„åˆ™
inhibit_rules:
  # å¦‚æœä¸»æœºå®•æœºï¼ŒæŠ‘åˆ¶è¯¥ä¸»æœºçš„æ‰€æœ‰å…¶ä»–å‘Šè­¦
  - source_match:
      alertname: 'InstanceDown'
    target_match_re:
      alertname: '(CPU|Memory|Disk)High'
    equal: ['instance']

  # å¦‚æœæ•°æ®åº“å®•æœºï¼ŒæŠ‘åˆ¶æ•°æ®åº“ç›¸å…³çš„å…¶ä»–å‘Šè­¦
  - source_match:
      alertname: 'DatabaseDown'
    target_match:
      service: 'database'
    equal: ['job']
```

### 6.3 è‡ªå®šä¹‰å‘Šè­¦é€šçŸ¥

```python
# backend/app/monitoring/alerts.py
import asyncio
import aiohttp
import logging
from typing import Dict, List, Any
from datetime import datetime

class AlertManager:
    def __init__(self):
        self.webhook_urls = {
            'slack': os.getenv('SLACK_WEBHOOK_URL'),
            'dingtalk': os.getenv('DINGTALK_WEBHOOK_URL'),
            'email': os.getenv('ALERT_EMAIL_URL')
        }

    async def send_alert(self, alert_data: Dict[str, Any], channels: List[str] = None):
        """å‘é€å‘Šè­¦é€šçŸ¥"""
        channels = channels or ['slack']

        for channel in channels:
            try:
                if channel == 'slack':
                    await self._send_slack_alert(alert_data)
                elif channel == 'dingtalk':
                    await self._send_dingtalk_alert(alert_data)
                elif channel == 'email':
                    await self._send_email_alert(alert_data)

            except Exception as e:
                logging.error(f"Failed to send {channel} alert: {e}")

    async def _send_slack_alert(self, alert_data: Dict[str, Any]):
        """å‘é€Slackå‘Šè­¦"""
        webhook_url = self.webhook_urls.get('slack')
        if not webhook_url:
            return

        color = {
            'critical': 'danger',
            'warning': 'warning',
            'info': 'good'
        }.get(alert_data.get('severity', 'info'), 'warning')

        payload = {
            "attachments": [{
                "color": color,
                "title": f"ğŸš¨ {alert_data.get('title', 'Alert')}",
                "fields": [
                    {"title": "ä¸¥é‡ç¨‹åº¦", "value": alert_data.get('severity', 'Unknown'), "short": True},
                    {"title": "æœåŠ¡", "value": alert_data.get('service', 'Unknown'), "short": True},
                    {"title": "æè¿°", "value": alert_data.get('description', 'No description'), "short": False},
                ],
                "footer": "AIå¹¿å‘Šä»£æŠ•ç³»ç»Ÿ",
                "ts": int(datetime.now().timestamp())
            }]
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(webhook_url, json=payload) as response:
                if response.status != 200:
                    raise Exception(f"Slack API error: {response.status}")

    async def _send_dingtalk_alert(self, alert_data: Dict[str, Any]):
        """å‘é€é’‰é’‰å‘Šè­¦"""
        webhook_url = self.webhook_urls.get('dingtalk')
        if not webhook_url:
            return

        emoji = {
            'critical': 'ğŸ”¥',
            'warning': 'âš ï¸',
            'info': 'â„¹ï¸'
        }.get(alert_data.get('severity', 'info'), 'âš ï¸')

        payload = {
            "msgtype": "markdown",
            "markdown": {
                "title": f"{emoji} ç³»ç»Ÿå‘Šè­¦",
                "text": f"""
## {alert_data.get('title', 'Alert')}

**ä¸¥é‡ç¨‹åº¦**: {alert_data.get('severity', 'Unknown')}
**æœåŠ¡**: {alert_data.get('service', 'Unknown')}
**æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

**æè¿°**:
{alert_data.get('description', 'No description')}

---

*AIå¹¿å‘Šä»£æŠ•ç³»ç»Ÿç›‘æ§*
                """
            }
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(webhook_url, json=payload) as response:
                if response.status != 200:
                    raise Exception(f"DingTalk API error: {response.status}")

    async def _send_email_alert(self, alert_data: Dict[str, Any]):
        """å‘é€é‚®ä»¶å‘Šè­¦"""
        # è¿™é‡Œå¯ä»¥é›†æˆé‚®ä»¶æœåŠ¡API
        logging.info(f"Email alert sent: {alert_data}")

# ä¸šåŠ¡å‘Šè­¦è§¦å‘å™¨
class BusinessAlertTrigger:
    def __init__(self, alert_manager: AlertManager):
        self.alert_manager = alert_manager

    async def check_and_trigger_alerts(self):
        """æ£€æŸ¥å¹¶è§¦å‘ä¸šåŠ¡å‘Šè­¦"""
        while True:
            try:
                await self._check_business_rules()
                await asyncio.sleep(300)  # æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            except Exception as e:
                logging.error(f"Error in business alert check: {e}")
                await asyncio.sleep(60)

    async def _check_business_rules(self):
        """æ£€æŸ¥ä¸šåŠ¡è§„åˆ™"""
        # æ£€æŸ¥æ˜¯å¦æœ‰é•¿æ—¶é—´æ— æ–°é¡¹ç›®
        await self._check_no_new_projects()

        # æ£€æŸ¥å……å€¼ç”³è¯·ç§¯å‹
        await self._check_pending_recharges()

        # æ£€æŸ¥å¼‚å¸¸ç™»å½•è¡Œä¸º
        await self._check_suspicious_logins()

    async def _check_no_new_projects(self):
        """æ£€æŸ¥æ˜¯å¦æœ‰é•¿æ—¶é—´æ— æ–°é¡¹ç›®"""
        # è¿™é‡Œåº”è¯¥æŸ¥è¯¢æ•°æ®åº“æˆ–æŒ‡æ ‡
        has_new_projects = await self._query_new_projects_last_24h()

        if not has_new_projects:
            await self.alert_manager.send_alert({
                'title': 'ä¸šåŠ¡é¢„è­¦ï¼š24å°æ—¶å†…æ— æ–°é¡¹ç›®',
                'description': 'ç³»ç»Ÿå·²ç»24å°æ—¶æ²¡æœ‰æ–°çš„é¡¹ç›®åˆ›å»ºï¼Œè¯·æ£€æŸ¥ä¸šåŠ¡æ˜¯å¦æ­£å¸¸',
                'severity': 'warning',
                'service': 'business'
            })

    async def _check_pending_recharges(self):
        """æ£€æŸ¥å……å€¼ç”³è¯·ç§¯å‹"""
        pending_amount = await self._query_pending_recharge_amount()

        if pending_amount > 50000:  # è¶…è¿‡5ä¸‡å…ƒ
            await self.alert_manager.send_alert({
                'title': 'å……å€¼ç”³è¯·ç§¯å‹',
                'description': f'å¾…å¤„ç†å……å€¼é‡‘é¢è¾¾åˆ° Â¥{pending_amount:,.2f}ï¼Œè¯·åŠæ—¶å¤„ç†',
                'severity': 'warning',
                'service': 'business'
            })

    async def _check_suspicious_logins(self):
        """æ£€æŸ¥å¼‚å¸¸ç™»å½•è¡Œä¸º"""
        failed_login_count = await self._query_failed_login_count_last_5min()

        if failed_login_count > 50:  # 5åˆ†é’Ÿå†…å¤±è´¥ç™»å½•è¶…è¿‡50æ¬¡
            await self.alert_manager.send_alert({
                'title': 'å¼‚å¸¸ç™»å½•è¡Œä¸º',
                'description': f'5åˆ†é’Ÿå†…å¤±è´¥ç™»å½•æ¬¡æ•°è¾¾åˆ° {failed_login_count} æ¬¡ï¼Œå¯èƒ½å­˜åœ¨æ”»å‡»è¡Œä¸º',
                'severity': 'critical',
                'service': 'security'
            })
```

---

## 7. æ•…éšœå¤„ç†

### 7.1 æ•…éšœå“åº”æµç¨‹

```python
# backend/app/monitoring/incident_manager.py
import asyncio
import logging
from enum import Enum
from datetime import datetime, timedelta
from typing import Dict, List, Optional

class IncidentSeverity(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class IncidentStatus(Enum):
    OPEN = "open"
    INVESTIGATING = "investigating"
    IDENTIFIED = "identified"
    MONITORING = "monitoring"
    RESOLVED = "resolved"

class Incident:
    def __init__(self,
                 id: str,
                 title: str,
                 description: str,
                 severity: IncidentSeverity,
                 service: str):
        self.id = id
        self.title = title
        self.description = description
        self.severity = severity
        self.service = service
        self.status = IncidentStatus.OPEN
        self.created_at = datetime.utcnow()
        self.updated_at = datetime.utcnow()
        self.assigned_to = None
        self.resolution = None
        self.tags = []

class IncidentManager:
    def __init__(self):
        self.active_incidents: Dict[str, Incident] = {}
        self.alert_manager = AlertManager()

    async def create_incident(self,
                           title: str,
                           description: str,
                           severity: IncidentSeverity,
                           service: str,
                           alert_data: Dict = None) -> Incident:
        """åˆ›å»ºæ•…éšœäº‹ä»¶"""
        incident_id = f"INC-{datetime.utcnow().strftime('%Y%m%d%H%M%S')}"

        incident = Incident(
            id=incident_id,
            title=title,
            description=description,
            severity=severity,
            service=service
        )

        if alert_data:
            incident.tags.extend(alert_data.get('tags', []))

        self.active_incidents[incident_id] = incident

        # å‘é€å‘Šè­¦é€šçŸ¥
        await self._send_incident_alert(incident)

        # è®°å½•åˆ°æ—¥å¿—
        logging.info(f"Incident created: {incident_id} - {title}")

        return incident

    async def update_incident_status(self,
                                   incident_id: str,
                                   status: IncidentStatus,
                                   update_message: str = None,
                                   assigned_to: str = None):
        """æ›´æ–°æ•…éšœçŠ¶æ€"""
        if incident_id not in self.active_incidents:
            logging.error(f"Incident {incident_id} not found")
            return

        incident = self.active_incidents[incident_id]
        old_status = incident.status
        incident.status = status
        incident.updated_at = datetime.utcnow()

        if assigned_to:
            incident.assigned_to = assigned_to

        # å‘é€çŠ¶æ€æ›´æ–°é€šçŸ¥
        await self._send_status_update(incident, old_status, update_message)

        logging.info(f"Incident {incident_id} status updated: {old_status.value} -> {status.value}")

    async def resolve_incident(self,
                             incident_id: str,
                             resolution: str):
        """è§£å†³æ•…éšœäº‹ä»¶"""
        if incident_id not in self.active_incidents:
            logging.error(f"Incident {incident_id} not found")
            return

        incident = self.active_incidents[incident_id]
        incident.status = IncidentStatus.RESOLVED
        incident.resolution = resolution
        incident.updated_at = datetime.utcnow()

        # å‘é€è§£å†³é€šçŸ¥
        await self._send_resolution_alert(incident)

        # å°†äº‹ä»¶ç§»åˆ°å†å²è®°å½•ï¼ˆåœ¨å®é™…å®ç°ä¸­åº”è¯¥æŒä¹…åŒ–ï¼‰
        del self.active_incidents[incident_id]

        logging.info(f"Incident {incident_id} resolved: {resolution}")

    async def _send_incident_alert(self, incident: Incident):
        """å‘é€æ•…éšœå‘Šè­¦"""
        severity_mapping = {
            IncidentSeverity.LOW: 'info',
            IncidentSeverity.MEDIUM: 'warning',
            IncidentSeverity.HIGH: 'warning',
            IncidentSeverity.CRITICAL: 'critical'
        }

        alert_data = {
            'title': f'ğŸš¨ æ•…éšœäº‹ä»¶: {incident.title}',
            'description': f"""
**æ•…éšœID**: {incident.id}
**æœåŠ¡**: {incident.service}
**ä¸¥é‡ç¨‹åº¦**: {incident.severity.value}
**åˆ›å»ºæ—¶é—´**: {incident.created_at.strftime('%Y-%m-%d %H:%M:%S')}

**æè¿°**:
{incident.description}

**å½±å“èŒƒå›´**:
å¾…è¯„ä¼°

**å½“å‰çŠ¶æ€**:
æ­£åœ¨è°ƒæŸ¥ä¸­...
            """,
            'severity': severity_mapping[incident.severity],
            'service': incident.service,
            'tags': ['incident', incident.service] + incident.tags
        }

        await self.alert_manager.send_alert(alert_data, ['slack', 'email'])

    async def _send_status_update(self, incident: Incident, old_status: IncidentStatus, message: str = None):
        """å‘é€çŠ¶æ€æ›´æ–°"""
        if old_status == incident.status:
            return

        alert_data = {
            'title': f'ğŸ“‹ æ•…éšœçŠ¶æ€æ›´æ–°: {incident.id}',
            'description': f"""
**æ•…éšœ**: {incident.title}
**çŠ¶æ€å˜æ›´**: {old_status.value} -> {incident.status.value}
**æ›´æ–°æ—¶é—´**: {incident.updated_at.strftime('%Y-%m-%d %H:%M:%S')}
"""
        }

        if message:
            alert_data['description'] += f"\n**å¤‡æ³¨**: {message}"

        if incident.assigned_to:
            alert_data['description'] += f"\n**è´Ÿè´£äºº**: {incident.assigned_to}"

        await self.alert_manager.send_alert(alert_data, ['slack'])

    async def _send_resolution_alert(self, incident: Incident):
        """å‘é€æ•…éšœè§£å†³é€šçŸ¥"""
        alert_data = {
            'title': f'âœ… æ•…éšœå·²è§£å†³: {incident.id}',
            'description': f"""
**æ•…éšœ**: {incident.title}
**è§£å†³æ—¶é—´**: {incident.updated_at.strftime('%Y-%m-%d %H:%M:%S')}
**æŒç»­æ—¶é—´**: {incident.updated_at - incident.created_at}

**è§£å†³æ–¹æ¡ˆ**:
{incident.resolution}
            """,
            'severity': 'info',
            'service': incident.service,
            'tags': ['resolved', incident.service]
        }

        await self.alert_manager.send_alert(alert_data, ['slack', 'email'])

    def get_active_incidents(self) -> List[Incident]:
        """è·å–æ´»è·ƒæ•…éšœåˆ—è¡¨"""
        return list(self.active_incidents.values())

    def get_incident_summary(self) -> Dict[str, int]:
        """è·å–æ•…éšœæ‘˜è¦ç»Ÿè®¡"""
        incidents = self.get_active_incidents()

        summary = {
            'total': len(incidents),
            'critical': 0,
            'high': 0,
            'medium': 0,
            'low': 0
        }

        for incident in incidents:
            summary[incident.severity.value] += 1

        return summary

# æ•…éšœå“åº”è‡ªåŠ¨åŒ–
class IncidentResponseAutomation:
    def __init__(self, incident_manager: IncidentManager):
        self.incident_manager = incident_manager

    async def auto_escalate_incidents(self):
        """è‡ªåŠ¨å‡çº§æ•…éšœ"""
        while True:
            try:
                await self._check_escalation_rules()
                await asyncio.sleep(300)  # æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            except Exception as e:
                logging.error(f"Error in incident escalation: {e}")
                await asyncio.sleep(60)

    async def _check_escalation_rules(self):
        """æ£€æŸ¥å‡çº§è§„åˆ™"""
        incidents = self.incident_manager.get_active_incidents()

        for incident in incidents:
            # è§„åˆ™1: ä¸¥é‡æ•…éšœ30åˆ†é’Ÿæœªå¤„ç†ï¼Œè‡ªåŠ¨å‡çº§
            if (incident.severity == IncidentSeverity.CRITICAL and
                incident.status == IncidentStatus.OPEN and
                datetime.utcnow() - incident.created_at > timedelta(minutes=30)):

                await self.incident_manager.update_incident_status(
                    incident.id,
                    IncidentStatus.INVESTIGATING,
                    "è‡ªåŠ¨å‡çº§ï¼šä¸¥é‡æ•…éšœè¶…è¿‡30åˆ†é’Ÿæœªå¤„ç†"
                )

                # å‘é€å‡çº§é€šçŸ¥
                await self._send_escalation_notification(incident)

            # è§„åˆ™2: æ•…éšœè¶…è¿‡2å°æ—¶æœªè§£å†³ï¼Œå‘é€æé†’
            if (datetime.utcnow() - incident.created_at > timedelta(hours=2) and
                incident.status not in [IncidentStatus.RESOLVED, IncidentStatus.MONITORING]):

                await self._send_reminder_notification(incident)

    async def _send_escalation_notification(self, incident: Incident):
        """å‘é€å‡çº§é€šçŸ¥"""
        alert_data = {
            'title': f'ğŸ”¥ æ•…éšœè‡ªåŠ¨å‡çº§: {incident.id}',
            'description': f"""
æ•…éšœ {incident.title} å·²è¶…è¿‡30åˆ†é’Ÿæœªå¤„ç†ï¼Œå·²è‡ªåŠ¨å‡çº§ã€‚

**æ•…éšœID**: {incident.id}
**ä¸¥é‡ç¨‹åº¦**: {incident.severity.value}
**åˆ›å»ºæ—¶é—´**: {incident.created_at.strftime('%Y-%m-%d %H:%M:%S')}

è¯·ç«‹å³å¤„ç†æ­¤æ•…éšœï¼
            """,
            'severity': 'critical',
            'service': incident.service
        }

        await self.incident_manager.alert_manager.send_alert(alert_data, ['slack', 'email'])

    async def _send_reminder_notification(self, incident: Incident):
        """å‘é€æé†’é€šçŸ¥"""
        duration = datetime.utcnow() - incident.created_at

        alert_data = {
            'title': f'â° æ•…éšœå¤„ç†æé†’: {incident.id}',
            'description': f"""
æ•…éšœ {incident.title} å·²æŒç»­ {duration}ã€‚

**æ•…éšœID**: {incident.id}
**å½“å‰çŠ¶æ€**: {incident.status.value}
**è´Ÿè´£äºº**: {incident.assigned_to or 'æœªåˆ†é…'}

è¯·åŠæ—¶è·Ÿè¿›å¤„ç†è¿›åº¦ã€‚
            """,
            'severity': 'warning',
            'service': incident.service
        }

        await self.incident_manager.alert_manager.send_alert(alert_data, ['slack'])
```

### 7.2 æ•…éšœå¤„ç†æ‰‹å†Œ

```markdown
# æ•…éšœå¤„ç†æ‰‹å†Œ (Playbook)

## 1. åº”ç”¨æœåŠ¡æ•…éšœ

### 1.1 æœåŠ¡æ— å“åº”
**ç—‡çŠ¶**: APIè¿”å›502/503é”™è¯¯ï¼Œå¥åº·æ£€æŸ¥å¤±è´¥

**æ’æŸ¥æ­¥éª¤**:
1. æ£€æŸ¥åº”ç”¨æœåŠ¡çŠ¶æ€
   ```bash
   docker-compose ps
   docker-compose logs backend
   ```

2. æ£€æŸ¥æœåŠ¡èµ„æºä½¿ç”¨
   ```bash
   docker stats
   ```

3. æ£€æŸ¥åº”ç”¨æ—¥å¿—
   ```bash
   docker-compose logs --tail=100 backend
   ```

4. æ£€æŸ¥ç«¯å£å ç”¨
   ```bash
   netstat -tlnp | grep :8000
   ```

**è§£å†³æªæ–½**:
- é‡å¯åº”ç”¨æœåŠ¡: `docker-compose restart backend`
- æ‰©å®¹æœåŠ¡: `docker-compose up -d --scale backend=3`
- å¦‚æœæŒç»­å¤±è´¥ï¼Œå›æ»šåˆ°ä¸Šä¸€ä¸ªç‰ˆæœ¬

### 1.2 æ•°æ®åº“è¿æ¥æ•…éšœ
**ç—‡çŠ¶**: æ•°æ®åº“ç›¸å…³é”™è¯¯ï¼Œè¿æ¥è¶…æ—¶

**æ’æŸ¥æ­¥éª¤**:
1. æ£€æŸ¥æ•°æ®åº“æœåŠ¡çŠ¶æ€
   ```bash
   docker-compose ps postgres
   docker-compose logs postgres
   ```

2. æ£€æŸ¥æ•°æ®åº“è¿æ¥æ•°
   ```sql
   SELECT count(*) FROM pg_stat_activity;
   ```

3. æ£€æŸ¥æ•°æ®åº“èµ„æºä½¿ç”¨
   ```bash
   docker exec -it postgres psql -U postgres -c "SELECT * FROM pg_stat_activity;"
   ```

**è§£å†³æªæ–½**:
- é‡å¯æ•°æ®åº“æœåŠ¡: `docker-compose restart postgres`
- æ¸…ç†ç©ºé—²è¿æ¥: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE state = 'idle';`
- æ‰©å®¹æ•°æ®åº“èµ„æº

### 1.3 Redisç¼“å­˜æ•…éšœ
**ç—‡çŠ¶**: ç¼“å­˜ç›¸å…³åŠŸèƒ½å¼‚å¸¸ï¼Œè®¤è¯å¤±è´¥

**æ’æŸ¥æ­¥éª¤**:
1. æ£€æŸ¥RedisæœåŠ¡çŠ¶æ€
   ```bash
   docker-compose ps redis
   docker-compose logs redis
   ```

2. æµ‹è¯•Redisè¿æ¥
   ```bash
   docker exec -it redis redis-cli ping
   ```

3. æ£€æŸ¥Rediså†…å­˜ä½¿ç”¨
   ```bash
   docker exec -it redis redis-cli info memory
   ```

**è§£å†³æªæ–½**:
- é‡å¯RedisæœåŠ¡: `docker-compose restart redis`
- æ¸…ç†Redisç¼“å­˜: `docker exec -it redis redis-cli FLUSHALL`
- æ‰©å®¹Rediså†…å­˜

## 2. ä¸šåŠ¡é€»è¾‘æ•…éšœ

### 2.1 æ•°æ®ä¸ä¸€è‡´
**ç—‡çŠ¶**: å‰åç«¯æ•°æ®æ˜¾ç¤ºä¸ä¸€è‡´

**æ’æŸ¥æ­¥éª¤**:
1. æ£€æŸ¥æ•°æ®åº“æ•°æ®å®Œæ•´æ€§
   ```sql
   SELECT COUNT(*) FROM projects;
   SELECT COUNT(*) FROM ad_accounts;
   SELECT project_id, COUNT(*) FROM ad_accounts GROUP BY project_id;
   ```

2. æ£€æŸ¥å¤–é”®çº¦æŸ
   ```sql
   SELECT conname, conrelid::regclass, confrelid::regclass
   FROM pg_constraint
   WHERE contype = 'f';
   ```

3. æ£€æŸ¥è§¦å‘å™¨çŠ¶æ€
   ```sql
   SELECT tgname, tgrelid::regclass, tgenabled
   FROM pg_trigger;
   ```

**è§£å†³æªæ–½**:
- è¿è¡Œæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥è„šæœ¬
- ä¿®å¤æŸåçš„æ•°æ®
- é‡æ–°åˆå§‹åŒ–ç›¸å…³è¡¨

### 2.2 æ€§èƒ½ä¸‹é™
**ç—‡çŠ¶**: å“åº”æ—¶é—´æ˜æ˜¾å¢åŠ 

**æ’æŸ¥æ­¥éª¤**:
1. æ£€æŸ¥æ…¢æŸ¥è¯¢æ—¥å¿—
   ```sql
   SELECT query, mean_time, calls
   FROM pg_stat_statements
   ORDER BY mean_time DESC
   LIMIT 10;
   ```

2. æ£€æŸ¥ç´¢å¼•ä½¿ç”¨æƒ…å†µ
   ```sql
   SELECT schemaname, tablename, indexname, idx_scan, idx_tup_read, idx_tup_fetch
   FROM pg_stat_user_indexes
   ORDER BY idx_scan DESC;
   ```

3. æ£€æŸ¥è¡¨å¤§å°å’Œè¡Œæ•°
   ```sql
   SELECT
       schemaname,
       tablename,
       pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
       n_tup_ins as inserts,
       n_tup_upd as updates,
       n_tup_del as deletes
   FROM pg_stat_user_tables
   ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
   ```

**è§£å†³æªæ–½**:
- ä¼˜åŒ–æ…¢æŸ¥è¯¢
- æ·»åŠ æˆ–é‡å»ºç´¢å¼•
- æ¸…ç†æˆ–å½’æ¡£å†å²æ•°æ®
- æ‰©å®¹æ•°æ®åº“èµ„æº

## 3. å®‰å…¨ç›¸å…³æ•…éšœ

### 3.1 å¤§é‡ç™»å½•å¤±è´¥
**ç—‡çŠ¶**: ç”¨æˆ·æ— æ³•æ­£å¸¸ç™»å½•ï¼Œå®‰å…¨å‘Šè­¦

**æ’æŸ¥æ­¥éª¤**:
1. æ£€æŸ¥ç™»å½•å¤±è´¥æ—¥å¿—
   ```bash
   grep "login failed" /var/log/ai-ad-spend/app.log | tail -50
   ```

2. æ£€æŸ¥å¼‚å¸¸IP
   ```bash
   grep "login failed" /var/log/ai-ad-spend/app.log | \
   grep -o '[0-9]\+\.[0-9]\+\.[0-9]\+\.[0-9]\+' | \
   sort | uniq -c | sort -nr | head -10
   ```

3. æ£€æŸ¥ç”¨æˆ·è´¦æˆ·çŠ¶æ€
   ```sql
   SELECT username, last_login, failed_login_attempts
   FROM users
   WHERE failed_login_attempts > 5;
   ```

**è§£å†³æªæ–½**:
- å¯åŠ¨é€Ÿç‡é™åˆ¶
- å°ç¦å¼‚å¸¸IPåœ°å€
- é”å®šå—æ”»å‡»çš„è´¦æˆ·
- å‘é€å®‰å…¨å‘Šè­¦

### 3.2 æ•°æ®æ³„éœ²é£é™©
**ç—‡çŠ¶**: æ•æ„Ÿæ•°æ®æ„å¤–æš´éœ²

**æ’æŸ¥æ­¥éª¤**:
1. æ£€æŸ¥è®¿é—®æ—¥å¿—
   ```bash
   grep -E "(password|token|secret)" /var/log/nginx/access.log
   ```

2. æ£€æŸ¥APIå“åº”
   ```bash
   curl -s "http://localhost:8000/api/users" | jq .
   ```

3. æ£€æŸ¥æƒé™é…ç½®
   ```sql
   SELECT schemaname, tablename, policyname, permissive, roles, cmd, qual
   FROM pg_policies;
   ```

**è§£å†³æªæ–½**:
- ç«‹å³ä¿®å¤æ•°æ®æš´éœ²ç‚¹
- æ›´æ–°æƒé™é…ç½®
- æ’¤é”€å¯ç–‘çš„è®¿é—®ä»¤ç‰Œ
- è¿›è¡Œå®‰å…¨å®¡è®¡

## 4. è¿ç»´æ“ä½œæµç¨‹

### 4.1 ç´§æ€¥å“åº”æµç¨‹
1. **æ¥æ”¶å‘Šè­¦** (0-5åˆ†é’Ÿ)
   - ç¡®è®¤å‘Šè­¦ä¿¡æ¯
   - åˆ›å»ºæ•…éšœäº‹ä»¶
   - é€šçŸ¥ç›¸å…³äººå‘˜

2. **åˆæ­¥è¯Šæ–­** (5-15åˆ†é’Ÿ)
   - æ£€æŸ¥æœåŠ¡çŠ¶æ€
   - åˆ†ææ—¥å¿—å’ŒæŒ‡æ ‡
   - ç¡®å®šå½±å“èŒƒå›´

3. **ç´§æ€¥å¤„ç†** (15-30åˆ†é’Ÿ)
   - å®æ–½ä¸´æ—¶è§£å†³æ–¹æ¡ˆ
   - æ¢å¤å…³é”®æœåŠ¡
   - ç¼“è§£æ•…éšœå½±å“

4. **æ·±å…¥åˆ†æ** (30-60åˆ†é’Ÿ)
   - æ‰¾å‡ºæ ¹æœ¬åŸå› 
   - åˆ¶å®šé•¿æœŸè§£å†³æ–¹æ¡ˆ
   - æ›´æ–°ç›‘æ§å‘Šè­¦

5. **æ¢å¤éªŒè¯** (60åˆ†é’Ÿ+)
   - éªŒè¯æœåŠ¡å®Œå…¨æ¢å¤
   - ç›‘æ§ç³»ç»Ÿç¨³å®šæ€§
   - é€šçŸ¥ç›¸å…³æ–¹æ•…éšœè§£å†³

### 4.2 å˜æ›´ç®¡ç†æµç¨‹
1. **å˜æ›´ç”³è¯·**
   - å¡«å†™å˜æ›´ç”³è¯·å•
   - è¿›è¡Œé£é™©è¯„ä¼°
   - è·å¾—å¿…è¦å®¡æ‰¹

2. **å˜æ›´å‡†å¤‡**
   - åˆ¶å®šè¯¦ç»†å®æ–½è®¡åˆ’
   - å‡†å¤‡å›æ»šæ–¹æ¡ˆ
   - å®‰æ’å˜æ›´çª—å£

3. **å˜æ›´å®æ–½**
   - æŒ‰è®¡åˆ’æ‰§è¡Œå˜æ›´
   - å®æ—¶ç›‘æ§ç³»ç»ŸçŠ¶æ€
   - è®°å½•å®æ–½è¿‡ç¨‹

4. **å˜æ›´éªŒè¯**
   - éªŒè¯åŠŸèƒ½æ­£å¸¸
   - æ£€æŸ¥æ€§èƒ½æŒ‡æ ‡
   - ç¡®è®¤æ— å‰¯ä½œç”¨

5. **å˜æ›´å…³é—­**
   - æ›´æ–°ç³»ç»Ÿæ–‡æ¡£
   - æ€»ç»“å˜æ›´ç»éªŒ
   - å…³é—­å˜æ›´è¯·æ±‚
```

---

## ğŸ“ è¿ç»´æ”¯æŒ

### è¿ç»´å›¢é˜Ÿè”ç³»
- **è¿ç»´è´Ÿè´£äºº**: ops@company.com
- **å€¼ç­å·¥ç¨‹å¸ˆ**: oncall@company.com
- **ç´§æ€¥æ•…éšœçƒ­çº¿**: +86-xxx-xxxx-xxxx

### ç›‘æ§é¢æ¿
- **åº”ç”¨ç›‘æ§**: https://grafana.yourdomain.com
- **åŸºç¡€è®¾æ–½ç›‘æ§**: https://prometheus.yourdomain.com
- **æ—¥å¿—æŸ¥è¯¢**: https://loki.yourdomain.com
- **å‘Šè­¦ç®¡ç†**: https://alertmanager.yourdomain.com

### è¿ç»´å·¥å…·
- **æœåŠ¡éƒ¨ç½²**: https://deploy.yourdomain.com
- **é…ç½®ç®¡ç†**: https://config.yourdomain.com
- **æ•…éšœè·Ÿè¸ª**: https://incidents.yourdomain.com
- **çŸ¥è¯†åº“**: https://kb.yourdomain.com

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-11-11
**ä¸‹æ¬¡å®¡æŸ¥**: ç›‘æ§æ¶æ„æ›´æ–°æ—¶
**ç»´æŠ¤è´£ä»»äºº**: è¿ç»´å›¢é˜Ÿè´Ÿè´£äºº